{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf77b864-cd58-4341-945f-99ee4f40de25",
   "metadata": {},
   "source": [
    "# Линейная регрессия \n",
    "(*реализация через градиентный спуск без сторонних библиотек*)\n",
    "\n",
    "Пусть у нас есть некоторая функция потерь $f:\\mathbb{X}\\times\\mathbb{Y}\\rightarrow\\mathbb{R}$ и матрица признаков $X$ c целевой переменной $y$, тогда функционал ошибки выглядит так: $$L(f,X,y)=\\frac{1}{N}\\sum_{i=1}^{N}{f(\\vec{x}_i,y_i)}$$\n",
    "В нашем случае $f(\\vec{x},y)=(\\langle{\\vec{x},w}\\rangle-y)^2$, а значит: $$L(f,X,y)=\\frac{1}{N}\\sum_{i=1}^{N}{(\\langle{\\vec{x}_i,w}\\rangle-y_i)^2}$$\n",
    "Реализуем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c70712c1-857e-4ac3-a61e-1ef7856fdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import csv,math,time\n",
    "\n",
    "def loss_function(w:List[float],x:List[float],y:float)->float:\n",
    "    return (sum(list(map(lambda a,b:a*b,w,x)))-y)**2\n",
    "\n",
    "def error_functionality(w:List[float],X:List[List[float]],y:List[float])->float:\n",
    "    sum=0\n",
    "    for i in range(len(X)):\n",
    "        sum+=loss_function(w,X[i],y[i])\n",
    "    return sum/len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f2a2b-c1ad-4ea2-a007-3e492923bcda",
   "metadata": {},
   "source": [
    "Загружаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c8df16b-d633-4456-b32d-8c616e29bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 обьектов:  [[254.5200092, 102.1921871], [351.2964474, 89.79305002], [36.83525135, 330.3657023], [346.4289733, 113.03682], [475.2232841, 320.1528027]]\n"
     ]
    }
   ],
   "source": [
    "data,X,y = [],[],[]\n",
    "with open('my_data.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        # Преобразуем каждый элемент строки в float\n",
    "        data.append([float(value) for value in row])\n",
    "        X.append(data[-1][:-1])\n",
    "        y.append(data[-1][-1])\n",
    "w=len(X[0])*[1]\n",
    "print('Первые 5 обьектов: ',data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1239520-2ac2-4396-a9fb-84b647cf629d",
   "metadata": {},
   "source": [
    "Функции, которые немного изменены для корректной работы с данными из прошлого дневника:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e6a1de9-650b-425b-beea-10cc8caca69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Реализация градиента\n",
    "def grad(f, w:List[float],X:List[List[float]],y:List[float], eps=10**(-5))->List[float]:\n",
    "    w_dev=[]\n",
    "    for i in range(len(w)):\n",
    "        w_1,w_2=w.copy(),w.copy()\n",
    "        w_1[i]-=eps\n",
    "        w_2[i]+=eps\n",
    "        w_dev.append((f(w_2,X,y)-f(w_1,X,y))/(2*eps))\n",
    "    return w_dev\n",
    "    \n",
    "#Реализация градиентного спуска\n",
    "def grad_desc(f,w:List[float],X:List[List[float]],y:List[float],learning_rate=0.01,iterations=100000)->List[float]:\n",
    "    for i in range(iterations):\n",
    "        w=list(map(lambda a,l,g:a-l*g,w,len(w)*[learning_rate],grad(f,w,X,y)))\n",
    "    return [round(i,5) for i in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd3c21-be39-4e72-89b7-abe7eb511da5",
   "metadata": {},
   "source": [
    "Минимизируем функционал ошибки с помощью градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8208beb5-9962-4fe6-9690-e1b883a4c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коэффициенты w: [0.78374]\n",
      "Потраченное время: 1.357 сек\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print('Коэффициенты w:',grad_desc(error_functionality,w,X,y,0.00001,1000))\n",
    "end=time.time()\n",
    "print('Потраченное время:', round(end-start,3),'сек')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe71405-0e83-4057-89ae-cb682f1e1b4a",
   "metadata": {},
   "source": [
    "Проверим коэффициенты библиотекой *sklearn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "886e7138-9647-4b08-a093-54ff71ddb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коэффициенты w: [0.78374364]\n",
      "Потраченное время: 0.001 сек\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression(fit_intercept=False)\n",
    "start=time.time()\n",
    "model.fit(X,y)\n",
    "print('Коэффициенты w:',model.coef_)\n",
    "end=time.time()\n",
    "print('Потраченное время:', round(end-start,3),'сек')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1a1d7-dcfa-498e-a49b-9eb3a87bf9ec",
   "metadata": {},
   "source": [
    "Заметим, что моя реализация очень медленная по сравнению с Линейной регрессией из *sklearn*, т.к. она написана на языке C, который гораздо быстрее Python. Всё равно предлагаю более быстрый подход.\n",
    "#### Матричное дифференцирование\n",
    "$$L(w)=\\frac{1}{N}\\sum_{i=1}^{N}{(\\langle{\\vec{x}_i,w}\\rangle-y_i)^2}$$\n",
    "$$\\nabla_w{L(w)}=\\frac{2}{N}\\sum_{i=1}^{N}{(\\langle{\\vec{x}_i,w}\\rangle-y_i)}\\vec{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b52a1b8-e2af-41db-a3bb-dbaa201ed6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коэффициенты w: [0.78374]\n",
      "Потраченное время: 1.171 сек\n"
     ]
    }
   ],
   "source": [
    "def analytical_grad(w:List[float],X:List[List[float]],y:List[float])->List[float]:\n",
    "    gradient=[0]*len(w)\n",
    "    for i in range(len(X)):\n",
    "        C=2*(sum(list(map(lambda a,b:a*b,w,X[i])))-y[i])/len(X)\n",
    "        gradient=list(map(lambda g,k,v:g+k*v,gradient,len(w)*[C],X[i]))\n",
    "    return gradient\n",
    "\n",
    "def analytical_grad_desc(w:List[float],X:List[List[float]],y:List[float],learning_rate=0.01,iterations=100000)->List[float]:\n",
    "    for i in range(iterations):\n",
    "        w=list(map(lambda a,l,g:a-l*g,w,len(w)*[learning_rate],analytical_grad(w,X,y)))\n",
    "    return [round(i,5) for i in w]\n",
    "\n",
    "w=len(X[0])*[1]\n",
    "start=time.time()\n",
    "print('Коэффициенты w:',analytical_grad_desc(w,X,y,0.00001,1000))\n",
    "end=time.time()\n",
    "print('Потраченное время:', round(end-start,3),'сек')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
